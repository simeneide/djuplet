#!/usr/bin/env python3
"""
Download and extract paragraphs from a specified Wikipedia dump.
Headings, lists, and non-text elements are excluded.
Outputs one JSON object per line in JSONL format:
{
  "url": <article URL>,
  "paragraph_number": <int>,
  "text": <paragraph text>
}

Example usage:
  python wiki_downloader.py --language nn --output_file nynorsk.jsonl --max_paragraphs 100 --minimum_words_paragraph 15

Dependencies:
  - requests (for downloading)
  - mwparserfromhell (for parsing wikitext)
     pip install mwparserfromhell
  - lxml (for streaming through XML)
     pip install lxml
  - tqdm (for progress bars)
     pip install tqdm

"""

import os
import bz2
import argparse
import json
import requests
import re
from lxml import etree
import mwparserfromhell
from tqdm import tqdm

def download_wiki_dump(language: str, output_path: str):
    """Download the latest Wikipedia pages-articles dump for a given language."""
    url = f"https://dumps.wikimedia.org/{language}wiki/latest/{language}wiki-latest-pages-articles.xml.bz2"
    print(f"[INFO] Downloading Wikipedia dump from: {url}")
    
    with requests.get(url, stream=True) as r:
        r.raise_for_status()
        total_size = int(r.headers.get('Content-Length', 0))
        chunk_size = 8192
        
        with open(output_path, 'wb') as f, tqdm(
                total=total_size, unit='B', unit_scale=True, desc='Downloading dump') as pbar:
            for chunk in r.iter_content(chunk_size=chunk_size):
                f.write(chunk)
                pbar.update(len(chunk))
    
    print(f"[INFO] Download complete: {output_path}")

def detect_namespace(bz2_file: str):
    """Detects the XML namespace in the Wikipedia dump."""
    with bz2.open(bz2_file, 'rb') as f:
        for event, elem in etree.iterparse(f, events=('start',)):
            if '}' in elem.tag:
                namespace = elem.tag.split('}')[0] + "}"
                print(f"[INFO] Detected namespace: {namespace}")
                return namespace
    return "{http://www.mediawiki.org/xml/export-0.10/}"  # Fallback

def is_valid_article(title: str):
    """Returns True if the page is a valid Wikipedia article."""
    if title.startswith(("Wikipedia:", "Kategori:", "Fil:", "Mal:", "Hjelp:", "MediaWiki:", "Brukar:", "Diskusjon:")):
        return False
    if title == "Hovudside":  # Skip main page
        return False
    return True


def extract_paragraphs_from_page(wiki_text: str, min_words: int):
    parsed = mwparserfromhell.parse(wiki_text)

    # 1. Remove templates FIRST (recursively)
    templates = list(parsed.ifilter_templates(recursive=True))
    for template in templates:
        parsed.remove(template)

    # 2. Remove File/Image links in SEPARATE pass
    file_links = [
        link for link in parsed.ifilter_wikilinks()
        if str(link.title).strip().startswith(("File:", "Image:"))
    ]
    for link in file_links:
        parsed.remove(link)

    # 3. Remove HTML tags in SEPARATE pass
    html_tags = list(parsed.ifilter_tags())
    for tag in html_tags:
        parsed.remove(tag)

    # 4. Remove headings in FINAL pass
    headings = list(parsed.ifilter_headings())
    for heading in headings:
        parsed.remove(heading)

    # Process remaining text
    plain_text = parsed.strip_code(normalize=False, collapse=False)
    split_paragraphs = re.split(r'\n{2,}', plain_text)
    raw_paragraphs = [re.sub(r'\s+', ' ', p.strip()) for p in split_paragraphs if p.strip()]

    valid_endings = {'.', '!', '?', ','}
    paragraphs = []
    for p in raw_paragraphs:
        words = p.split()
        if len(words) < min_words:
            continue
        if p[-1] not in valid_endings:
            continue
        if "..." in p or "â€¦" in p or "thumb|" in p:
            continue
        paragraphs.append(p)

    return paragraphs


def process_dump(language: str, bz2_file: str, output_file: str, max_paragraphs: int, min_words: int):
    """
    Streams through the Wikipedia dump XML (bz2_file), extracts paragraphs,
    and writes them line-by-line in JSONL format to output_file.
    Stops processing when max_paragraphs is reached.
    """
    print(f"[INFO] Processing dump: {bz2_file}")
    print(f"[INFO] Writing JSONL paragraphs to: {output_file}")

    namespace = detect_namespace(bz2_file)
    total_paragraphs = 0

    with bz2.open(bz2_file, 'rb') as f, open(output_file, 'w', encoding='utf-8') as out_f:
        context = etree.iterparse(f, events=('end',), tag=f'{namespace}page')

        with tqdm(desc='Parsing pages', unit=' pages') as pbar:
            for event, elem in context:
                title_el = elem.find(f'{namespace}title')
                revision_el = elem.find(f'{namespace}revision')

                if title_el is not None and revision_el is not None:
                    title = title_el.text if title_el.text else ""

                    # Skip non-articles
                    if not is_valid_article(title):
                        elem.clear()
                        continue

                    text_el = revision_el.find(f'{namespace}text')
                    if text_el is not None and text_el.text:
                        paragraphs = extract_paragraphs_from_page(text_el.text, min_words)

                        # Construct Wikipedia URL
                        url_title = title.replace(' ', '_')
                        page_url = f"https://{language}.wikipedia.org/wiki/{url_title}"

                        for idx, p in enumerate(paragraphs):
                            record = {
                                "url": page_url,
                                "paragraph_number": idx + 1,
                                "text": p
                            }
                            out_f.write(json.dumps(record, ensure_ascii=False) + "\n")
                            total_paragraphs += 1

                            if total_paragraphs >= max_paragraphs:
                                print(f"[INFO] Reached max_paragraphs limit ({max_paragraphs}). Stopping.")
                                return

                    pbar.update(1)  # Update progress bar

                # Clear memory
                elem.clear()
                while elem.getprevious() is not None:
                    del elem.getparent()[0]

    print(f"[INFO] Total paragraphs extracted: {total_paragraphs}")

def main():
    parser = argparse.ArgumentParser(
        description="Download a Wikipedia dump for a given language and extract paragraphs to JSON lines."
    )
    parser.add_argument("--language", type=str, required=True, help="Wikipedia language code (e.g., en, de, fr, no).")
    parser.add_argument("--output_file", type=str, required=True, help="Output file path for the resulting JSON lines.")
    parser.add_argument("--temp_dump_file", type=str, default="temp_wiki_dump.xml.bz2",
                        help="Path to store or read the downloaded Wikipedia dump (bz2).")
    parser.add_argument("--max_paragraphs", type=int, default=10_000_000,
                        help="Maximum number of paragraphs to extract (default: 10M).")
    parser.add_argument("--minimum_words_paragraph", type=int, default=15,
                        help="Minimum words per paragraph (default: 15).")

    args = parser.parse_args()

    # ðŸ”¹ **Check if the Wikipedia dump exists; if not, download it**
    if not os.path.exists(args.temp_dump_file) or os.path.getsize(args.temp_dump_file) == 0:
        print(f"[INFO] Wikipedia dump not found. Downloading: {args.temp_dump_file}")
        download_wiki_dump(args.language, args.temp_dump_file)

    # ðŸ”¹ **Process the Wikipedia dump**
    process_dump(args.language, args.temp_dump_file, args.output_file, args.max_paragraphs, args.minimum_words_paragraph)

    print("[INFO] Done.")

if __name__ == "__main__":
    main()

